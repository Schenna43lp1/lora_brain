import os
import subprocess
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ------------------------------------------------------------
# EINSTELLUNGEN
# ------------------------------------------------------------
# WICHTIG: BASE_MODEL muss mit dem Modell √ºbereinstimmen, mit dem der LoRA-Adapter trainiert wurde!
BASE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"  # Laut adapter_config.json
MODEL_DIR = Path(r"D:\dataset\trained_model")
EXPORT_DIR = Path(r"D:\dataset\final_ollama")
MODEL_NAME = "markusbrainlora"  # Name f√ºr Ollama und GGUF
# ------------------------------------------------------------

EXPORT_DIR.mkdir(parents=True, exist_ok=True)

print("üî§ Lade Tokenizer‚Ä¶")
try:
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
except Exception as e:
    print(f"‚ùå Fehler beim Laden des Tokenizers: {e}")
    sys.exit(1)

print("üß† Lade Base-Modell‚Ä¶")
try:
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype="auto",
        device_map="cpu",
        local_files_only=False
    )
except Exception as e:
    print(f"‚ùå Fehler beim Laden des Base-Modells: {e}")
    sys.exit(1)

adapter_config = MODEL_DIR / "adapter_config.json"

# ------------------------------------------------------------
# Pr√ºfen ob LoRA oder Full Model
# ------------------------------------------------------------
if adapter_config.exists():
    print("üîó LoRA erkannt ‚Üí Merging‚Ä¶")
    try:
        model = PeftModel.from_pretrained(
            model,
            str(MODEL_DIR),
            local_files_only=True
        )
        model = model.merge_and_unload()
        print("‚úÖ LoRA Adapter erfolgreich gemerged!")
    except Exception as e:
        print(f"‚ùå Fehler beim Mergen der LoRA-Adapter: {e}")
        sys.exit(1)
else:
    print("‚ö†Ô∏è Keine LoRA-Dateien gefunden.")
    print("‚û°Ô∏è Lade Modell als FULL HF Modell‚Ä¶")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            str(MODEL_DIR),
            torch_dtype="auto",
            device_map="cpu",
            local_files_only=True
        )
    except Exception as e:
        print(f"‚ùå Fehler beim Laden des Full Models: {e}")
        sys.exit(1)

# ------------------------------------------------------------
# SAVE MERGED MODEL
# ------------------------------------------------------------
MERGED_DIR = EXPORT_DIR / "merged_model"
MERGED_DIR.mkdir(parents=True, exist_ok=True)

print("üíæ Speichere gemerged Model‚Ä¶")
try:
    model.save_pretrained(str(MERGED_DIR))
    tokenizer.save_pretrained(str(MERGED_DIR))
    print(f"‚úÖ Modell gespeichert in: {MERGED_DIR}")
except Exception as e:
    print(f"‚ùå Fehler beim Speichern des Modells: {e}")
    sys.exit(1)

# ------------------------------------------------------------
# EXPORT GGUF (Ben√∂tigt llama.cpp)
# ------------------------------------------------------------
GGUF_OUT = EXPORT_DIR / f"{MODEL_NAME}.gguf"

print("\nüì¶ Konvertiere nach GGUF‚Ä¶")
print("‚ö†Ô∏è HINWEIS: Dies ben√∂tigt llama.cpp installiert!")
print("   Installiere llama.cpp mit: pip install llama-cpp-python")
print("   Oder klone: git clone https://github.com/ggerganov/llama.cpp\n")

# Versuche mit llama.cpp convert.py
convert_script = Path("llama.cpp/convert.py")
if convert_script.exists():
    try:
        result = subprocess.run([
            sys.executable,
            str(convert_script),
            str(MERGED_DIR),
            "--outfile",
            str(GGUF_OUT),
            "--outtype",
            "f16"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"‚úÖ GGUF erfolgreich erstellt: {GGUF_OUT}")
        else:
            print(f"‚ö†Ô∏è GGUF Konvertierung fehlgeschlagen:")
            print(result.stderr)
    except Exception as e:
        print(f"‚ö†Ô∏è GGUF Konvertierung √ºbersprungen: {e}")
        print("   ‚Üí Du kannst das Modell manuell mit llama.cpp konvertieren")
else:
    print("‚ö†Ô∏è llama.cpp/convert.py nicht gefunden - √ºberspringe GGUF Konvertierung")
    print(f"   ‚Üí Merged Model verf√ºgbar in: {MERGED_DIR}")

# ------------------------------------------------------------
# OLLAMA Modellfile (nur wenn GGUF existiert)
# ------------------------------------------------------------
if GGUF_OUT.exists():
    modelfile = f"""FROM {MODEL_NAME}.gguf
TEMPLATE \"\"\"You are {MODEL_NAME}. Respond clearly.
{{{{ .Prompt }}}}\"\"\"
PARAMETER temperature 0.4
PARAMETER top_p 0.9
"""

    modelfile_path = EXPORT_DIR / "Modelfile"
    try:
        with open(modelfile_path, "w", encoding="utf-8") as f:
            f.write(modelfile)
        print(f"‚úÖ Modelfile erstellt: {modelfile_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Fehler beim Erstellen des Modelfiles: {e}")

    print("\nüê™ Registriere Modell in Ollama‚Ä¶")
    try:
        result = subprocess.run([
            "ollama",
            "create",
            MODEL_NAME,
            "-f",
            str(modelfile_path)
        ], capture_output=True, text=True, cwd=str(EXPORT_DIR))
        
        if result.returncode == 0:
            print("‚úÖ Modell in Ollama registriert!")
            print(f"\nüéâ Fertig! Starte mit:")
            print(f"üëâ ollama run {MODEL_NAME}")
        else:
            print(f"‚ö†Ô∏è Ollama Registrierung fehlgeschlagen:")
            print(result.stderr)
            print(f"\nManuell registrieren mit:")
            print(f"cd {EXPORT_DIR}")
            print(f"ollama create {MODEL_NAME} -f Modelfile")
    except FileNotFoundError:
        print("‚ö†Ô∏è Ollama nicht gefunden. Installiere Ollama von https://ollama.ai")
    except Exception as e:
        print(f"‚ö†Ô∏è Fehler bei Ollama-Registrierung: {e}")
else:
    print("\n‚ö†Ô∏è GGUF-Datei nicht gefunden - Ollama-Registrierung √ºbersprungen")

print(f"\n‚úÖ Prozess abgeschlossen!")
print(f"üìÅ Merged Model: {MERGED_DIR}")
if GGUF_OUT.exists():
    print(f"üì¶ GGUF: {GGUF_OUT}")